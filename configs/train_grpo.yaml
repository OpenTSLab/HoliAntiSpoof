defaults:
  - global: qwen2_5_omni
  - data@data_dict: spoofing
  - model: qwen2_5_omni
  - override hydra/job_logging: none
  - _self_

global:
  train_type: grpo

metric:
  _target_: qwenvl.train.metrics.RewardWrapper

data_dict:
  train:
    dataset_list:
      - data/sine/train.json
      - data/codecfake_ntu/train.json
      - data/vctk/train.json
      - data/partial_edit/train.json
    dataset_max_samples: 5000
  val:
    dataset_list:
      - data/sine/val.json
      - data/codecfake_ntu/val.json
      - data/partial_edit/val.json
      - data/vctk/val.json
    dataset_max_samples: 500

train_dataset: ${data_dict.train}

val_dataset: ${data_dict.val}

data_collator:
  _target_: qwenvl.data.data_qwen.OmniCollator
  torchify_keys:
    - video_second_per_grid
    - spoof_embed

reward_fn:
  _target_: qwenvl.train.reward_functions.JSONSpoofReward

model:
  lora_ckpt: experiments/all_data/r_64/checkpoint-20000

# training args
trainer:
  # deepspeed: scripts/zero/zero0.json # ZeRO stage 0 - 1. bf16, when rewards within a group is the same, advantage=0, loss=0, assertion error from `assert all_groups_norm > 0` 2. fp16: gradient cannot be back-propagated, don't know why
  bf16: true

  output_dir: experiments/all_data/r_64_grpo
  run_name: "r_64_grpo"
  report_to: "tensorboard"

  per_device_train_batch_size: 8
  per_device_eval_batch_size: 16
  dataloader_num_workers: 12

  learning_rate: !!float 2e-6
  adam_beta1: 0.9
  adam_beta2: 0.95
  weight_decay: 0.1
  warmup_ratio: 0.05
  max_grad_norm: 1
  gradient_accumulation_steps: 1
  gradient_checkpointing: true

  lr_scheduler_type: "cosine"

  num_train_epochs: 1
  max_steps: 10000

  eval_strategy: steps
  eval_steps: 1000
  metric_for_best_model: "reward"
  greater_is_better: true

  save_strategy: best
  save_steps: 2000
  save_total_limit: 3

  logging_steps: 10

  # for GRPO compatibility
  include_for_metrics:
    - loss
  prediction_loss_only: true
  dataloader_drop_last: true

  # GRPO args
  use_liger_loss: false
  num_iterations: 1
  num_generations: 8
  max_completion_length: 256
  loss_type: "dr_grpo"
  temperature: 1.5
  top_p: 0.9
  beta: 0.01
  delta: 1.3
  epsilon_high: 0.28
  unused_items_for_generation: ["keywords", "prompt", "ref", "audio", "video", "index"]
  shuffle_dataset: true
  mask_truncated_completions: true
  

hydra:
  output_subdir: null
  run:
    dir: .
